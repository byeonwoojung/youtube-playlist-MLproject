# Wav2wav2ëŠ” ì›ì‹œíŒŒí˜•(ì‹œê°„ì— ë”°ë¥¸ ì§„í­)ìœ¼ë¡œ ë¶„ì„
# ì˜¤ë””ì˜¤ ì •ëŸ‰ì  íŠ¹ì„±ì€ ê¸°ì¤€ì„ ì„¸ì›Œ ë¶„ì„

import os
import subprocess
import pandas as pd
import numpy as np
import librosa
from tqdm import tqdm
from urllib.parse import urlparse, parse_qs
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import torch
import warnings
import threading
import signal
import sys
import faulthandler
import multiprocessing as mp
from itertools import islice
import gc

# ì„¸ê·¸í´íŠ¸ ë°©ì§€ ì„¤ì •
faulthandler.enable()

def segfault_handler(sig, frame):
    faulthandler.dump_traceback()
    print("Segmentation fault detected, cleaning up...")
    sys.exit(1)

signal.signal(signal.SIGSEGV, segfault_handler)

# ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” ë©€í‹°í”„ë¡œì„¸ì‹± ì„¤ì •
def setup_batch_optimized_multiprocessing():
    """ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”ë¥¼ ìœ„í•œ ì„¤ì •"""
    try:
        mp.set_start_method('spawn', force=True)
        
        # ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” í™˜ê²½ ë³€ìˆ˜
        os.environ.update({
            "OMP_NUM_THREADS": "4",
            "MKL_NUM_THREADS": "4",
            "NUMEXPR_NUM_THREADS": "4",
            "OPENBLAS_NUM_THREADS": "4",
            "TOKENIZERS_PARALLELISM": "false",
            "PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:256",
            "CUDA_LAUNCH_BLOCKING": "0",
            "PYTHONWARNINGS": "ignore:semaphore_tracker:UserWarning"
        })
        
        print("ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” ì„¤ì • ì™„ë£Œ[2][4]")
        return True
    except Exception as e:
        print(f"ë©€í‹°í”„ë¡œì„¸ì‹± ì„¤ì • ì‹¤íŒ¨: {e}")
        return False

setup_batch_optimized_multiprocessing()

# ë©”ëª¨ë¦¬ ìµœì í™” ê°•í™”
def setup_memory_optimization():
    try:
        gc.disable()
        gc.set_threshold(0)
        print("ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ[2][3]")
        return True
    except Exception as e:
        print(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
        return False

setup_memory_optimization()

# CSV ì €ì¥ ì‹œ ë©”ëª¨ë¦¬ ë¦¬ì…‹ í•¨ìˆ˜
def reset_memory_after_csv_save():
    try:
        # 1. íŒŒì´ì¬ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.enable()
        collected = gc.collect()
        gc.disable()
        
        # 2. GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            torch.cuda.ipc_collect()
        
        # 3. NumPy ë©”ëª¨ë¦¬ ì •ë¦¬
        import numpy as np
        np.seterr(all='ignore')
        
        print(f"ë©”ëª¨ë¦¬ ë¦¬ì…‹ ì™„ë£Œ: {collected}ê°œ ê°ì²´ ì •ë¦¬")
        return True
        
    except Exception as e:
        print(f"ë©”ëª¨ë¦¬ ë¦¬ì…‹ ì‹¤íŒ¨: {e}")
        return False

# CSV ì „ì²´ ì†Œìˆ˜ì  ì •ë°€ë„ ì €ì¥ ì„¤ì •
def save_csv_with_full_precision(df, filepath):
    try:
        #  ê¸°ë³¸ pandas ì €ì¥ìœ¼ë¡œ ì „ì²´ ì •ë°€ë„ ìœ ì§€
        df.to_csv(filepath, index=False, encoding="utf-8-sig")
        print(f"ì „ì²´ ì •ë°€ë„ CSV ì €ì¥: {filepath}")
        return True
    except Exception as e:
        print(f"CSV ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

# CSV ì „ì²´ ì •ë°€ë„ ì½ê¸° í•¨ìˆ˜
def read_csv_with_full_precision(filepath):
    try:
        # float_precision='round_trip'ì€ ì½ê¸°ì—ì„œë§Œ ì‚¬ìš©[1]
        df = pd.read_csv(filepath, float_precision='round_trip')
        print(f"ì „ì²´ ì •ë°€ë„ CSV ì½ê¸°: {filepath}")
        return df
    except Exception as e:
        print(f"CSV ì½ê¸° ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

# transformers ì•ˆì „ ì„í¬íŠ¸
try:
    from transformers import (
        Wav2Vec2ForSequenceClassification, 
        Wav2Vec2FeatureExtractor,
        AutoFeatureExtractor
    )
    TRANSFORMERS_AVAILABLE = True
    print("GPU ê°ì • ë¶„ì„ ëª¨ë¸ ì„í¬íŠ¸ ì„±ê³µ")
except ImportError as e:
    print(f"Transformers ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    TRANSFORMERS_AVAILABLE = False

warnings.filterwarnings('ignore')

# íš¨ìœ¨ì ì¸ í´ë” êµ¬ì¡°
SAVE_DIR = "./youtube/temp_audio"
TEMP_DIR = "./youtube/tempAudio"  
CPU_OUTPUT_FILE = "./youtube/cpu_audio_features.csv"
GPU_OUTPUT_FILE = "./youtube/gpu_emotion_analysis.csv"
FINAL_OUTPUT_FILE = "./youtube/final_merged_analysis.csv"
CACHE_DIR = "./youtube/cache"

# ë‹¨ê³„ë³„ ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì • (ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­)
CPU_BATCH_SIZE = 15
GPU_BATCH_SIZE = 5
SAVE_BATCH_SIZE = 15

for directory in [SAVE_DIR, TEMP_DIR, CACHE_DIR]:
    os.makedirs(directory, exist_ok=True)

# GPU ì„¤ì • ìµœì í™”
def setup_batch_gpu():
    try:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        if torch.cuda.is_available():
            torch.cuda.init()
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
            torch.cuda.memory.set_per_process_memory_fraction(0.8)
            
            print(f"ë°°ì¹˜ ì²˜ë¦¬ GPU ì„¤ì •: {torch.cuda.get_device_name(0)}[6]")
        else:
            print("CUDA ì‚¬ìš© ë¶ˆê°€, CPU ëª¨ë“œ")
        return device
    except Exception as e:
        print(f"GPU ì„¤ì • ì‹¤íŒ¨: {e}")
        return torch.device('cpu')

device = setup_batch_gpu()

class AccurateSevenEmotionMusicAnalyzer:    
    def __init__(self):
        self.models = {}
        self.is_loaded = False
        self.inference_lock = threading.Lock()
        
        # ì •í™•í•œ 7ê°€ì§€ ê°ì • ë¼ë²¨ë§ (ê³µì‹ ì •ë³´ ê¸°ë°˜)
        self.emotion_mapping = {
            'happy': 0, 'sad': 1, 'angry': 2, 'fear': 3, 'surprise': 4, 'disgust': 5, 'neutral': 6
        }
        
        # ì—­ë§¤í•‘ (ë¼ë²¨ â†’ ê°ì •ëª…)
        self.label_to_emotion = {v: k for k, v in self.emotion_mapping.items()}
        
        # ê°ì •ëª… ë§¤í•‘ (í•œêµ­ì–´-ì˜ì–´)
        self.emotion_korean_mapping = {
            'happy': 'ê¸°ì¨', 'sad': 'ìŠ¬í””', 'angry': 'ë¶„ë…¸', 
            'fear': 'ê³µí¬', 'surprise': 'ë†€ëŒ', 'disgust': 'í˜ì˜¤', 'neutral': 'ì¤‘ë¦½'
        }
        
        print(f"ì •í™•í•œ 7ê°€ì§€ ê°ì • ë¶„ë¥˜ê¸° ì´ˆê¸°í™” (ì „ì²´ ì •ë°€ë„):")
        print(f"ê°ì •: {list(self.emotion_mapping.keys())}")
    
    def initialize_emotion_models(self):
        try:
            if not TRANSFORMERS_AVAILABLE:
                print("Transformers ì—†ìŒ")
                return False
                
            print("7ê°€ì§€ ê°ì • ë¶„ë¥˜ GPU ëª¨ë¸ ë¡œë”©...")
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            try:
                # CPUì—ì„œ ë¨¼ì € ë¡œë“œ í›„ GPUë¡œ ì´ë™
                wav2vec2_model = Wav2Vec2ForSequenceClassification.from_pretrained(
                    "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition",
                    torch_dtype=torch.float32,
                    low_cpu_mem_usage=False,
                    cache_dir=CACHE_DIR,
                    ignore_mismatched_sizes=True,
                    device_map=None,
                )
                
                # í”„ë¡œì„¸ì„œ ë¡œë”©
                try:
                    wav2vec2_processor = AutoFeatureExtractor.from_pretrained(
                        "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition",
                        cache_dir=CACHE_DIR
                    )
                except:
                    wav2vec2_processor = Wav2Vec2FeatureExtractor.from_pretrained(
                        "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition",
                        cache_dir=CACHE_DIR
                    )
                
                # ì•ˆì „í•˜ê²Œ GPUë¡œ ì´ë™
                if torch.cuda.is_available():
                    wav2vec2_model = wav2vec2_model.to(device)
                
                self.models['wav2vec2'] = {
                    'model': wav2vec2_model,
                    'processor': wav2vec2_processor,
                    'emotions': ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised'],
                    'enabled': True
                }
                self.models['wav2vec2']['model'].eval()
                
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                
                print("7ê°€ì§€ ê°ì • ë¶„ë¥˜ GPU ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
                self.is_loaded = True
                return True
                
            except Exception as model_error:
                print(f"ëª¨ë¸ ë¡œë”© ì„¸ë¶€ ì˜¤ë¥˜: {model_error}")
                self.models['wav2vec2'] = {'enabled': False}
                return False
            
        except Exception as e:
            print(f"ê°ì • ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.models['wav2vec2'] = {'enabled': False}
            return False
    
    def enhanced_emotion_classification_with_musical_analysis(self, audio_features, gpu_emotion_result):
        try:
            # ì˜¤ë””ì˜¤ íŠ¹ì„± ë¶„ì„
            pitch_mean = audio_features.get('pitch_mean', 0) or 0
            energy_mean = audio_features.get('energy_mean', 0) or 0
            centroid_mean = audio_features.get('centroid_mean', 1000) or 1000
            bmp = audio_features.get('bmp', 120) or 120
            speech_rate = audio_features.get('speech_rate', 0) or 0
            initial_silence = audio_features.get('initial_silence', 0) or 0
            
            gpu_emotion = gpu_emotion_result.get('emotion_name', 'neutral')
            gpu_confidence = gpu_emotion_result.get('confidence', 0.5)
            
            print(f"ìŒì•…ì  íŠ¹ì„± ë¶„ì„: í”¼ì¹˜={pitch_mean}, ì—ë„ˆì§€={energy_mean}, BMP={bmp}")
            
            # í´ë˜ì‹/OST (ìŠ¬í””)
            sadness_indicators = {
                'low_energy': energy_mean < 0.15,          # ë‚®ì€ ì—ë„ˆì§€
                'minor_key_pitch': pitch_mean and 140 <= pitch_mean <= 155,  # ë‹¨ì¡° ìŒê³„ íŠ¹ì„±
                'slow_tempo': bmp < 70,                    # ëŠë¦° í…œí¬
                'instrumental': speech_rate < 10,          # ì•…ê¸° ì¤‘ì‹¬
                'contemplative_centroid': 500 <= centroid_mean <= 650,  # ì‚¬ìƒ‰ì ì¸ ìŒìƒ‰
                'emotional_build': initial_silence > 0.1   # ê°ì •ì  ì‹œì‘
            }

            # ê° ê¸°ì¤€ì— í•´ë‹¹í•˜ëŠ” ê²ƒì˜ ë¹„ìœ¨ì„ scoreë¡œ ì •í•¨
            sadness_score = sum(sadness_indicators.values()) / len(sadness_indicators)
            
            # í–‰ë³µ ê°ì • íŠ¹í™” ë¶„ë¥˜
            happiness_indicators = {
                'high_energy': energy_mean > 0.4,          # ë†’ì€ ì—ë„ˆì§€
                'major_key': pitch_mean and pitch_mean > 160,  # ì¥ì¡° ìŒê³„
                'upbeat_tempo': bmp > 100,                 # ë¹ ë¥¸ í…œí¬
                'bright_timbre': centroid_mean > 2000,     # ë°ì€ ìŒìƒ‰
                'rhythmic': speech_rate > 15              # ë¦¬ë“œë¯¸ì»¬í•œ íŠ¹ì„±
            }
            
            happiness_score = sum(happiness_indicators.values()) / len(happiness_indicators)
            
            # ë¶„ë…¸ ê°ì • íŠ¹í™” ë¶„ë¥˜
            anger_indicators = {
                'very_high_energy': energy_mean > 0.6,     # ë§¤ìš° ë†’ì€ ì—ë„ˆì§€
                'aggressive_timbre': centroid_mean > 3000, # ê³µê²©ì ì¸ ìŒìƒ‰
                'fast_aggressive_tempo': bmp > 130,        # ë¹ ë¥´ê³  ê³µê²©ì ì¸ í…œí¬
                'high_vocal_intensity': speech_rate > 20,  # ë†’ì€ ë³´ì»¬ ê°•ë„
                'sudden_dynamics': initial_silence < 0.05  # ê°‘ì‘ìŠ¤ëŸ¬ìš´ ì‹œì‘
            }
            
            anger_score = sum(anger_indicators.values()) / len(anger_indicators)
            
            # ê³µí¬ ê°ì • íŠ¹í™” ë¶„ë¥˜
            fear_indicators = {
                'tense_energy': 0.3 <= energy_mean <= 0.6, # ê¸´ì¥ëœ ì—ë„ˆì§€
                'dissonant_pitch': pitch_mean and pitch_mean > 180,  # ë¶ˆí˜‘í™”ìŒ
                'unstable_tempo': 80 <= bmp <= 140,        # ë¶ˆì•ˆì •í•œ í…œí¬
                'eerie_timbre': 1800 <= centroid_mean <= 2500,  # ìœ¼ìŠ¤ìŠ¤í•œ ìŒìƒ‰
                'minimal_vocal': speech_rate < 5           # ìµœì†Œí•œì˜ ë³´ì»¬
            }
            
            fear_score = sum(fear_indicators.values()) / len(fear_indicators)
            
            # ë†€ëŒ ê°ì • íŠ¹í™” ë¶„ë¥˜
            surprise_indicators = {
                'sudden_energy': energy_mean > 0.5,        # ê°‘ì‘ìŠ¤ëŸ¬ìš´ ì—ë„ˆì§€
                'sharp_timbre': centroid_mean > 2500,      # ë‚ ì¹´ë¡œìš´ ìŒìƒ‰
                'varied_tempo': bmp and (bmp < 60 or bmp > 160),  # ê·¹ë‹¨ì ì¸ í…œí¬
                'dynamic_vocal': speech_rate > 25,         # ì—­ë™ì ì¸ ë³´ì»¬
                'abrupt_start': initial_silence < 0.02     # ê°‘ì‘ìŠ¤ëŸ¬ìš´ ì‹œì‘
            }
            
            surprise_score = sum(surprise_indicators.values()) / len(surprise_indicators)
            
            # í˜ì˜¤ ê°ì • íŠ¹í™” ë¶„ë¥˜
            disgust_indicators = {
                'unpleasant_energy': 0.2 <= energy_mean <= 0.4,  # ë¶ˆì¾Œí•œ ì—ë„ˆì§€
                'harsh_timbre': centroid_mean > 3500,      # ê±°ì¹œ ìŒìƒ‰
                'irregular_tempo': bmp and (70 <= bmp <= 90),  # ë¶ˆê·œì¹™í•œ í…œí¬
                'distorted_vocal': speech_rate > 30,       # ì™œê³¡ëœ ë³´ì»¬
                'uncomfortable_start': 0.1 <= initial_silence <= 0.3  # ë¶ˆí¸í•œ ì‹œì‘
            }
            
            disgust_score = sum(disgust_indicators.values()) / len(disgust_indicators)
            
            # ì¤‘ë¦½ ê°ì • íŠ¹í™” ë¶„ë¥˜
            neutral_indicators = {
                'moderate_energy': 0.15 <= energy_mean <= 0.35,  # ë³´í†µ ì—ë„ˆì§€
                'balanced_timbre': 1000 <= centroid_mean <= 2000,  # ê· í˜•ì¡íŒ ìŒìƒ‰
                'steady_tempo': 90 <= bmp <= 120,          # ì•ˆì •ëœ í…œí¬
                'balanced_vocal': 5 <= speech_rate <= 15,  # ê· í˜•ì¡íŒ ë³´ì»¬
                'normal_start': 0.05 <= initial_silence <= 0.2  # ì¼ë°˜ì ì¸ ì‹œì‘
            }
            
            neutral_score = sum(neutral_indicators.values()) / len(neutral_indicators)
            
            # ìµœì¢… ê°ì • ê²°ì •
            emotion_scores = {
                'sad': sadness_score,
                'happy': happiness_score,
                'angry': anger_score,
                'fear': fear_score,
                'surprise': surprise_score,
                'disgust': disgust_score,
                'neutral': neutral_score
            }
            
            # ìŒì•…ì  íŠ¹ì„± ê¸°ë°˜ ìµœê³  ì ìˆ˜ ê°ì •
            best_musical_emotion = max(emotion_scores, key=emotion_scores.get)
            best_musical_score = emotion_scores[best_musical_emotion]
            
            print(f"ìŒì•…ì  ë¶„ì„ ê²°ê³¼: {best_musical_emotion}({best_musical_score}), GPU: {gpu_emotion}({gpu_confidence})")
            
            # ìµœì¢… ê°ì • ê²°ì • ë¡œì§
            final_emotion = gpu_emotion
            final_confidence = gpu_confidence
            
            # ìŒì•…ì  íŠ¹ì„±ì´ ê°•í•˜ê²Œ ë‚˜íƒ€ë‚˜ëŠ” ê²½ìš° (70% ì´ìƒ) ìŒì•…ì  ë¶„ì„ ìš°ì„ 
            if best_musical_score >= 0.7:
                final_emotion = best_musical_emotion
                final_confidence = min(0.95, best_musical_score + 0.1)
                print(f"ìŒì•…ì  íŠ¹ì„± ìš°ì„  ì ìš©: {final_emotion} (ì ìˆ˜: {best_musical_score})")
            
            # ìŠ¬í”” ê°ì • ê°•í™”
            elif sadness_score >= 0.5 and energy_mean < 0.12:  # ë§¤ìš° ë‚®ì€ ì—ë„ˆì§€ + ìŠ¬í”” íŠ¹ì„±
                final_emotion = 'sad'
                final_confidence = min(0.90, sadness_score + 0.2)
                print(f"ìŠ¬í”” ê°ì • íŠ¹ë³„ ì¼€ì´ìŠ¤ ì ìš©: {final_emotion} (ì ìˆ˜: {sadness_score})")
            
            # GPUì™€ ìŒì•…ì  ë¶„ì„ì´ ì¼ì¹˜í•˜ëŠ” ê²½ìš° ì‹ ë¢°ë„ í–¥ìƒ
            elif best_musical_emotion == gpu_emotion:
                final_confidence = min(0.95, (best_musical_score + gpu_confidence) / 2 + 0.1)
                print(f"GPU+ìŒì•… ì¼ì¹˜: {final_emotion} (ì‹ ë¢°ë„ í–¥ìƒ: {final_confidence})")
            
            # ì „ì²´ ì •ë°€ë„ ì ìš©
            final_emotion_label = self.emotion_mapping.get(final_emotion, 6)
            
            return {
                'emotion_name': final_emotion,
                'emotion_label': final_emotion_label,
                'confidence': final_confidence,
                'musical_analysis': {
                    'best_emotion': best_musical_emotion,
                    'scores': emotion_scores
                }
            }
            
        except Exception as e:
            print(f"ì •í™•í•œ ê°ì • ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
            return {
                'emotion_name': 'neutral',
                'emotion_label': 6,
                'confidence': 0.5,
                'musical_analysis': {'error': str(e)}
            }
    
    def cpu_extract_audio_features(self, y, sr):
        try:
            features = {}
            
            # ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ë° ê²€ì¦
            if len(y) < sr * 0.1:
                print(f"ì˜¤ë””ì˜¤ ê¸¸ì´ ë¶€ì¡±: {len(y)/sr:.2f}ì´ˆ")
                return self._get_default_features()
            
            # ë…¸ì´ì¦ˆ ì œê±° ë° ì •ê·œí™”
            y_filtered = self._preprocess_audio(y, sr)
            
            print(f"ì˜¤ë””ì˜¤ ì •ë³´: ê¸¸ì´={len(y)/sr:.2f}ì´ˆ, ìƒ˜í”Œë§ìœ¨={sr}Hz")
            
            def extract_pitch():
                try:
                    # YIN ì•Œê³ ë¦¬ì¦˜
                    f0_yin = librosa.yin(y_filtered, 
                                    fmin=librosa.note_to_hz('C2'), 
                                    fmax=librosa.note_to_hz('C7'), 
                                    sr=sr)
                    
                    if not np.all(np.isnan(f0_yin)):
                        valid_f0 = f0_yin[~np.isnan(f0_yin)]
                        if len(valid_f0) > len(f0_yin) * 0.1:
                            result = float(np.mean(valid_f0))
                            print(f"ğŸ¼ YIN í”¼ì¹˜: {result:.2f}Hz")
                            return result
                    
                    # PYIN
                    print("YIN ì‹¤íŒ¨, PYIN ì‹œë„")
                    f0_pyin, voiced_flag, voiced_probs = librosa.pyin(
                        y_filtered, 
                        fmin=librosa.note_to_hz('C2'), 
                        fmax=librosa.note_to_hz('C7'), 
                        sr=sr,
                        threshold=0.1
                    )
                    
                    if not np.all(np.isnan(f0_pyin)):
                        # í™•ì‹ ë„ ë†’ì€ í”¼ì¹˜ë§Œ ì‚¬ìš©
                        confident_pitch = f0_pyin[voiced_probs > 0.5]
                        if len(confident_pitch) > 0:
                            result = float(np.mean(confident_pitch))
                            print(f"PYIN í”¼ì¹˜: {result:.2f}Hz")
                            return result
                    
                    # Piptrack
                    print("PYIN ì‹¤íŒ¨, piptrack ì‹œë„")
                    pitches, magnitudes = librosa.piptrack(
                        y=y_filtered, sr=sr, 
                        fmin=librosa.note_to_hz('C2'), 
                        fmax=librosa.note_to_hz('C7'),
                        threshold=0.1
                    )
                    
                    pitch_values = []
                    for t in range(pitches.shape[1]):
                        index = magnitudes[:, t].argmax()
                        pitch = pitches[index, t]
                        if pitch > 0 and magnitudes[index, t] > 0.1:
                            pitch_values.append(pitch)
                    
                    if len(pitch_values) > 5:
                        result = float(np.median(pitch_values))
                        print(f"piptrack í”¼ì¹˜: {result:.2f}Hz")
                        return result
                    
                    print("ëª¨ë“  í”¼ì¹˜ ì¶”ì¶œ ë°©ë²• ì‹¤íŒ¨")
                    return None
                    
                except Exception as e:
                    print(f"í”¼ì¹˜ ì¶”ì¶œ ì˜¤ë¥˜: {type(e).__name__}: {e}")
                    return None
            
            def extract_energy():
                try:
                    # RMS ì—ë„ˆì§€
                    rms = librosa.feature.rms(y=y_filtered, frame_length=2048, hop_length=512)
                    rms_mean = float(np.mean(rms))
                    
                    # Zero Crossing Rate
                    zcr = librosa.feature.zero_crossing_rate(y_filtered)
                    zcr_mean = float(np.mean(zcr))
                    
                    # ì¡°í•© ì—ë„ˆì§€ ì§€í‘œ
                    combined_energy = rms_mean * (1 + zcr_mean)
                    
                    print(f"ì—ë„ˆì§€: RMS={rms_mean:.4f}, ZCR={zcr_mean:.4f}, ì¡°í•©={combined_energy:.4f}")
                    return combined_energy
                    
                except Exception as e:
                    print(f"ì—ë„ˆì§€ ì¶”ì¶œ ì˜¤ë¥˜: {type(e).__name__}: {e}")
                    return 0.0
            
            def extract_centroid():
                try:
                    # ê¸°ë³¸ ìŠ¤í™íŠ¸ëŸ´ ì„¼íŠ¸ë¡œì´ë“œ
                    centroid = librosa.feature.spectral_centroid(y=y_filtered, sr=sr)
                    centroid_mean = float(np.mean(centroid))
                    
                    # ìŠ¤í™íŠ¸ëŸ´ ë¡¤ì˜¤í”„ (ë³´ì™„ ì§€í‘œ)
                    rolloff = librosa.feature.spectral_rolloff(y=y_filtered, sr=sr, roll_percent=0.85)
                    rolloff_mean = float(np.mean(rolloff))
                    
                    # ê°€ì¤‘ í‰ê·  (ì„¼íŠ¸ë¡œì´ë“œ 70%, ë¡¤ì˜¤í”„ 30%)
                    weighted_centroid = centroid_mean * 0.7 + rolloff_mean * 0.3
                    
                    print(f"ì„¼íŠ¸ë¡œì´ë“œ: ê¸°ë³¸={centroid_mean:.1f}Hz, ë¡¤ì˜¤í”„={rolloff_mean:.1f}Hz, ê°€ì¤‘={weighted_centroid:.1f}Hz")
                    return weighted_centroid
                    
                except Exception as e:
                    print(f"ì„¼íŠ¸ë¡œì´ë“œ ì¶”ì¶œ ì˜¤ë¥˜: {type(e).__name__}: {e}")
                    return 1000.0
            
            def extract_bmp():
                try:
                    # ê¸°ë³¸ beat tracking
                    tempo, beats = librosa.beat.beat_track(y=y_filtered, sr=sr, hop_length=512)
                    
                    print(f"ê¸°ë³¸ í…œí¬: {tempo:.1f} BPM")
                    
                    # í™•ì¥ëœ ë²”ìœ„ (40-300 BPM)
                    if 40 <= tempo <= 300:
                        return float(tempo)
                    
                    # Onset detection ê¸°ë°˜
                    print("í…œí¬ ë²”ìœ„ ì´ˆê³¼, onset ê¸°ë°˜ ë°©ë²• ì‹œë„")
                    onset_frames = librosa.onset.onset_detect(
                        y=y_filtered, sr=sr, units='time', 
                        backtrack=True, normalize=True
                    )
                    
                    if len(onset_frames) > 3:
                        intervals = np.diff(onset_frames)
                        median_interval = np.median(intervals)
                        valid_intervals = intervals[
                            (intervals > median_interval * 0.5) & 
                            (intervals < median_interval * 2.0)
                        ]
                        
                        if len(valid_intervals) > 0:
                            avg_interval = np.mean(valid_intervals)
                            onset_bpm = 60.0 / avg_interval if avg_interval > 0 else None
                            
                            if onset_bpm and 40 <= onset_bpm <= 300:
                                print(f"onset í…œí¬: {onset_bpm:.1f} BPM")
                                return float(onset_bpm)
                    
                    # í…œí¬ê·¸ë¨ ê¸°ë°˜
                    print("onset ì‹¤íŒ¨, í…œí¬ê·¸ë¨ ì‹œë„")
                    tempogram = librosa.feature.tempogram(y=y_filtered, sr=sr)
                    tempo_freqs = librosa.tempo_frequencies(len(tempogram), sr=sr)
                    
                    # ê°€ì¥ ê°•í•œ í…œí¬ ì£¼íŒŒìˆ˜ ì°¾ê¸°
                    tempo_strength = np.mean(tempogram, axis=1)
                    max_tempo_idx = np.argmax(tempo_strength)
                    tempogram_bpm = tempo_freqs[max_tempo_idx] * 60
                    
                    if 40 <= tempogram_bpm <= 300:
                        print(f"í…œí¬ê·¸ë¨ í…œí¬: {tempogram_bpm:.1f} BPM")
                        return float(tempogram_bpm)
                    
                    print("ëª¨ë“  í…œí¬ ì¶”ì¶œ ë°©ë²• ì‹¤íŒ¨")
                    return None
                    
                except Exception as e:
                    print(f"BMP ì¶”ì¶œ ì˜¤ë¥˜: {type(e).__name__}: {e}")
                    return None
            
            def extract_speech_rate():
                try:
                    # ë‹¨ìœ„ ì‹œê°„ë‹¹ onset ìˆ˜
                    onset_frames = librosa.onset.onset_detect(
                        y=y_filtered, sr=sr, units='time',
                        delta=0.05,
                        backtrack=True
                    )
                    
                    duration = len(y_filtered) / sr
                    speech_rate = len(onset_frames) / duration * 60
                    
                    print(f"Speech Rate: {speech_rate:.1f} onsets/min")
                    return int(speech_rate)
                    
                except Exception as e:
                    print(f"Speech Rate ì¶”ì¶œ ì˜¤ë¥˜: {type(e).__name__}: {e}")
                    return 0
            
            def extract_initial_silence():
                try:
                    # ë” ì„¸ë°€í•œ ì—ë„ˆì§€ ë¶„ì„
                    frame_length = 1024
                    hop_length = 256
                    
                    rms = librosa.feature.rms(
                        y=y_filtered, 
                        frame_length=frame_length, 
                        hop_length=hop_length
                    )
                    rms_vals = rms[0]
                    
                    # ë™ì  ì„ê³„ê°’ ê³„ì‚°
                    overall_rms = np.mean(rms_vals)
                    threshold = max(0.01, overall_rms * 0.05)
                    
                    # ì—°ì†ëœ í”„ë ˆì„ì—ì„œ ì„ê³„ê°’ ì´ˆê³¼í•˜ëŠ” ì§€ì  ì°¾ê¸°
                    above_threshold = rms_vals > threshold
                    
                    # ìµœì†Œ 3í”„ë ˆì„ ì—°ì†ìœ¼ë¡œ ì„ê³„ê°’ ì´ˆê³¼í•˜ëŠ” ì§€ì 
                    for i in range(len(above_threshold) - 2):
                        if np.all(above_threshold[i:i+3]):
                            silence_duration = librosa.frames_to_time(
                                i, sr=sr, hop_length=hop_length
                            )
                            print(f"ì´ˆê¸° ë¬´ìŒ: {silence_duration:.3f}ì´ˆ")
                            return float(silence_duration)
                    
                    # ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” í”„ë ˆì„ì´ ì—†ìœ¼ë©´ ì „ì²´ ê¸¸ì´
                    duration = len(y_filtered) / sr
                    print(f"ì „ì²´ ë¬´ìŒ: {duration:.3f}ì´ˆ")
                    return min(60.0, duration)
                    
                except Exception as e:
                    print(f"ì´ˆê¸° ë¬´ìŒ ì¶”ì¶œ ì˜¤ë¥˜: {type(e).__name__}: {e}")
                    return 0.0
            
            # ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” CPU ë³‘ë ¬ íŠ¹ì„± ì¶”ì¶œ
            with ThreadPoolExecutor(max_workers=6) as executor:
                futures = {
                    'pitch': executor.submit(extract_pitch),
                    'energy': executor.submit(extract_energy),
                    'centroid': executor.submit(extract_centroid),
                    'bmp': executor.submit(extract_bmp),
                    'speech': executor.submit(extract_speech_rate),
                    'silence': executor.submit(extract_initial_silence)
                }
                
                results = {}
                for key, future in futures.items():
                    try:
                        results[key] = future.result(timeout=10)
                    except Exception as e:
                        print(f"{key} ì¶”ì¶œ íƒ€ì„ì•„ì›ƒ/ì˜¤ë¥˜: {e}")
                        default_values = {
                            'pitch': None, 'energy': 0.0, 'centroid': 1000.0,
                            'bmp': None, 'speech': 0, 'silence': 0.0
                        }
                        results[key] = default_values[key]
            
            return {
                'pitch_mean': results['pitch'],
                'energy_mean': results['energy'],
                'centroid_mean': results['centroid'],
                'bmp': results['bmp'],
                'speech_rate': results['speech'],
                'initial_silence': results['silence']
            }
            
        except Exception as e:
            print(f"CPU íŠ¹ì„± ì¶”ì¶œ ì‹¤íŒ¨: {type(e).__name__}: {e}")
            return self._get_default_features()

    def _preprocess_audio(self, y, sr):
        try:
            # 1. DC ì„±ë¶„ ì œê±°
            y_filtered = y - np.mean(y)
            
            # 2. ì •ê·œí™”
            if np.max(np.abs(y_filtered)) > 0:
                y_filtered = y_filtered / np.max(np.abs(y_filtered))
            
            # 3. ê°„ë‹¨í•œ ì €ì—­ í†µê³¼ í•„í„°
            from scipy import signal
            nyquist = sr / 2
            cutoff = min(8000, nyquist * 0.8)
            b, a = signal.butter(3, cutoff / nyquist, btype='low')
            y_filtered = signal.filtfilt(b, a, y_filtered)
            
            return y_filtered
            
        except Exception as e:
            print(f"ì „ì²˜ë¦¬ ì‹¤íŒ¨, ì›ë³¸ ì‚¬ìš©: {e}")
            return y

    def _get_default_features(self):
        return {
            'pitch_mean': None, 
            'energy_mean': 0.0, 
            'centroid_mean': 1000.0,
            'bmp': None, 
            'speech_rate': 0, 
            'initial_silence': 0.0
        }
    
    def gpu_emotion_analysis(self, audio_path):
        if not self.models.get('wav2vec2', {}).get('enabled', False):
            return {'emotion_name': 'neutral', 'emotion_label': 6, 'confidence': 0.5}
        
        with self.inference_lock:
            try:
                model_info = self.models['wav2vec2']
                model = model_info['model']
                processor = model_info['processor']
                
                # ì•ˆì „í•œ ì˜¤ë””ì˜¤ ë¡œë“œ
                command = ["ffmpeg", "-y", "-i", audio_path, "-f", "f32le", "-ac", "1", "-ar", "16000", "-"]
                process = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, check=True, timeout=8)
                y = np.frombuffer(process.stdout, np.float32)
                
                if len(y) == 0:
                    raise Exception("ë¹ˆ ì˜¤ë””ì˜¤")
                
                # ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” ê¸¸ì´
                target_length = 16000 * 60
                if len(y) > target_length:
                    y = y[:target_length]
                elif len(y) < target_length:
                    y = np.pad(y, (0, target_length - len(y)), mode='constant')
                
                # GPU ì¶”ë¡ 
                inputs = processor(y, sampling_rate=16000, return_tensors="pt", padding=True)
                inputs = {key: value.to(device) for key, value in inputs.items()}
                
                with torch.no_grad():
                    try:
                        if torch.cuda.is_available():
                            with torch.amp.autocast('cuda'):
                                outputs = model(**inputs)
                        else:
                            outputs = model(**inputs)
                        
                        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
                        scores = predictions[0].cpu().numpy()
                        
                    except RuntimeError as e:
                        # GPU ì˜¤ë¥˜ ì‹œ CPU fallback
                        print(f"GPU ì˜¤ë¥˜, CPUë¡œ fallback: {e}")
                        inputs = {key: value.cpu() for key, value in inputs.items()}
                        model_cpu = model.cpu()
                        outputs = model_cpu(**inputs)
                        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
                        scores = predictions[0].numpy()
                        model.to(device)  # ëª¨ë¸ ë‹¤ì‹œ GPUë¡œ
                
                # 7ê°€ì§€ ê°ì • ë§¤í•‘
                emotions = model_info['emotions']
                emotion_mapping_local = {
                    'angry': 'angry', 'calm': 'neutral', 'disgust': 'disgust',
                    'fearful': 'fear', 'happy': 'happy', 'neutral': 'neutral',
                    'sad': 'sad', 'surprised': 'surprise'
                }
                
                emotion_scores = {}
                for i, emotion in enumerate(emotions):
                    mapped_emotion = emotion_mapping_local.get(emotion, 'neutral')
                    if mapped_emotion in emotion_scores:
                        emotion_scores[mapped_emotion] += float(scores[i])
                    else:
                        emotion_scores[mapped_emotion] = float(scores[i])
                
                best_emotion = max(emotion_scores, key=emotion_scores.get)
                confidence = emotion_scores[best_emotion]
                emotion_label = self.emotion_mapping.get(best_emotion, 6)
                
                return {
                    'emotion_name': best_emotion,
                    'emotion_label': emotion_label,
                    'confidence': confidence
                }
                
            except Exception as e:
                print(f"GPU ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
                return {'emotion_name': 'neutral', 'emotion_label': 6, 'confidence': 0.5}
            finally:
                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

# ì „ì—­ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤
music_emotion_analyzer = AccurateSevenEmotionMusicAnalyzer()

def extract_video_id(url):
    try:
        parsed = urlparse(url)
        return parse_qs(parsed.query).get("v", [None])[0]
    except:
        return None

# ì¸ë±ìŠ¤ ìˆœì„œ ê¸°ë°˜ URL ë¡œë“œ í•¨ìˆ˜
def load_urls_in_index_order():
    try:
        input_file = './youtube/allYoutubeInfo_themeFiltered.csv'
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(input_file):
            print(f"ì…ë ¥ íŒŒì¼ ì—†ìŒ: {input_file}")
            return []
        
        # ì¸ë±ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ë¡œë“œ
        df = pd.read_csv(input_file)
        df_sorted = df.sort_index()
        
        print(f"ì›ë³¸ CSV ë¡œë“œ: {len(df_sorted):,}ê°œ (ì¸ë±ìŠ¤ ìˆœì„œ ì •ë ¬)")
        
        # URLê³¼ ì¸ë±ìŠ¤ë¥¼ í•¨ê»˜ ë°˜í™˜
        url_list = []
        for idx, row in df_sorted.iterrows():
            video_url = row.get('video_url')
            if pd.notna(video_url):
                video_id = extract_video_id(video_url)
                if video_id:
                    url_list.append({
                        'index': idx,
                        'url': video_url,
                        'video_id': video_id
                    })
        
        print(f"ìœ íš¨í•œ URL: {len(url_list):,}ê°œ (ì¸ë±ìŠ¤ ìˆœì„œ ìœ ì§€)")
        return url_list
        
    except Exception as e:
        print(f"URL ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []

# ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€ ê°•í™” í•¨ìˆ˜
def get_processed_video_ids(output_file):
    try:
        if os.path.exists(output_file):
            df = read_csv_with_full_precision(output_file)
            processed_ids = set(df['video_id'].dropna().astype(str))
            print(f"ì´ë¯¸ ì²˜ë¦¬ëœ ID: {len(processed_ids)}ê°œ (from {output_file})")
            return processed_ids
        else:
            print(f"ê¸°ì¡´ ê²°ê³¼ íŒŒì¼ ì—†ìŒ: {output_file}")
            return set()
    except Exception as e:
        print(f"ì²˜ë¦¬ëœ ID í™•ì¸ ì‹¤íŒ¨: {e}")
        return set()

def safe_download_with_cookies(url, video_id):
    """ì•ˆì „í•œ ë¸Œë¼ìš°ì € ì¿ í‚¤ ë‹¤ìš´ë¡œë“œ (ë°°ì¹˜ ìµœì í™”)[6]"""
    output_path = os.path.join(SAVE_DIR, f"{video_id}.m4a")
    
    strategies = [
        # {
        #     'name': 'ios_client',  # DRM ìš°íšŒ ìµœì í™”
        #     'command': [
        #         "yt-dlp", url,
        #         "--cookies-from-browser", "firefox",
        #         "--extractor-args", "youtube:player_client=ios",
        #         "--user-agent", "Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X)",
        #         "--sleep-interval", "3",
        #         "-f", "bestaudio[ext=m4a]",
        #         "--download-sections", "*0-60",
        #         "--socket-timeout", "30",
        #         "--retries", "1",
        #         "-o", f"{SAVE_DIR}/{video_id}.%(ext)s",
        #         "--quiet"
        #     ]
        # },
        # {
        #     'name': 'android_fallback',
        #     'command': [
        #         "yt-dlp", url,
        #         "--cookies-from-browser", "firefox",
        #         "--extractor-args", "youtube:player_client=android",
        #         "--user-agent", "Mozilla/5.0 (Linux; Android 11; Pixel 5)",
        #         "--sleep-interval", "3",
        #         "-f", "bestaudio[ext=m4a]",
        #         "--download-sections", "*0-60",
        #         "--socket-timeout", "30",
        #         "--retries", "1",
        #         "-o", f"{SAVE_DIR}/{video_id}.%(ext)s",
        #         "--quiet"
        #     ]
        # },
        {
            'name': 'chrome',
            'command': [
                "yt-dlp", url,
                "--cookies-from-browser", "chrome",
                "--user-agent", "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36",
                "-f", "bestaudio[ext=m4a]",
                "--download-sections", "*0-60",
                "--socket-timeout", "30",
                "--retries", "1",
                "-o", f"{SAVE_DIR}/{video_id}.%(ext)s",
                "--quiet"
            ]
        }
    ]
    
    for strategy in strategies:
        try:
            subprocess.run(strategy['command'], check=True, timeout=60)
            if os.path.exists(output_path) and os.path.getsize(output_path) > 1024:
                return True
        except:
            continue
    return False

def cpu_load_audio_ffmpeg(path, sr=22050):
    try:
        command = ["ffmpeg", "-y", "-i", path, "-f", "f32le", "-ac", "1", "-ar", str(sr), "-threads", "8", "-"]
        process = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL, check=True, timeout=6)
        audio_data = np.frombuffer(process.stdout, np.float32)
        return audio_data.copy(), sr
    except Exception as e:
        raise Exception(f"CPU ì˜¤ë””ì˜¤ ë¡œë”© ì‹¤íŒ¨: {e}")

# CPU ë‹¤ìš´ë¡œë“œ + ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
def process_cpu_audio_features_only(url_data):
    url = url_data['url']
    video_id = url_data['video_id']
    index = url_data['index']
    
    result = {
        "index": index,
        "video_id": video_id, "url": url, "pitch_mean": None, "energy_mean": None,
        "centroid_mean": None, "bmp": None, "speech_rate": None, "initial_silence": None,
        "error": None
    }
    
    output_path = os.path.join(SAVE_DIR, f"{video_id}.m4a")

    try:
        # ë‹¤ìš´ë¡œë“œ
        if not safe_download_with_cookies(url, video_id):
            raise Exception("ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")

        # CPU: ì˜¤ë””ì˜¤ íŠ¹ì„± ì¶”ì¶œ
        y, sr = cpu_load_audio_ffmpeg(output_path)
        audio_features = music_emotion_analyzer.cpu_extract_audio_features(y, sr)
        
        # CPU íŠ¹ì„±ì„ ê²°ê³¼ì— ì €ì¥
        for key, value in audio_features.items():
            if key in result:
                result[key] = value

        # íŒŒì¼ ì‚­ì œí•˜ì§€ ì•Šê³  ìœ ì§€
        print(f"{video_id}(idx:{index}): CPU íŠ¹ì„± ì™„ë£Œ (íŒŒì¼ ìœ ì§€) - í”¼ì¹˜={result['pitch_mean']}, ì—ë„ˆì§€={result['energy_mean']}")

    except Exception as e:
        result["error"] = str(e)
        print(f"{video_id}(idx:{index}): CPU ì²˜ë¦¬ ì‹¤íŒ¨ - {str(e)}")
        # ì—ëŸ¬ ì‹œì—ë§Œ íŒŒì¼ ì‚­ì œ
        try:
            if os.path.exists(output_path):
                os.remove(output_path)
        except:
            pass

    return result

# GPU ê°ì • ë¶„ì„ (ê¸°ì¡´ íŒŒì¼ ì‚¬ìš©)
def process_gpu_emotion_only(video_data):
    video_id = video_data['video_id']
    output_path = os.path.join(SAVE_DIR, f"{video_id}.m4a")
    
    result = {
        "video_id": video_id, "emotion_name": None, "emotion_label": None, "confidence": None, "error": None
    }
    
    try:
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(output_path):
            raise Exception("ì˜¤ë””ì˜¤ íŒŒì¼ ì—†ìŒ")
        
        # ê°ì • ë¶„ì„
        gpu_emotion = music_emotion_analyzer.gpu_emotion_analysis(output_path)
        
        # 7ê°€ì§€ ê°ì • ë¶„ë¥˜ (ìŒì•…ì  íŠ¹ì„± í†µí•©)
        audio_features = {
            'pitch_mean': video_data.get('pitch_mean'),
            'energy_mean': video_data.get('energy_mean'),
            'centroid_mean': video_data.get('centroid_mean'),
            'bmp': video_data.get('bmp'),
            'speech_rate': video_data.get('speech_rate'),
            'initial_silence': video_data.get('initial_silence')
        }
        
        final_emotion = music_emotion_analyzer.enhanced_emotion_classification_with_musical_analysis(
            audio_features, gpu_emotion
        )
        
        result["emotion_name"] = final_emotion['emotion_name']
        result["emotion_label"] = final_emotion['emotion_label']
        result["confidence"] = final_emotion['confidence']

        print(f"{video_id}: GPU ê°ì • ì™„ë£Œ - {final_emotion['emotion_name']}({final_emotion['confidence']})")

    except Exception as e:
        result["error"] = str(e)
        print(f"{video_id}: GPU ì²˜ë¦¬ ì‹¤íŒ¨ - {str(e)}")
    finally:
        # GPU ì²˜ë¦¬ ì™„ë£Œ í›„ íŒŒì¼ ì‚­ì œ
        try:
            if os.path.exists(output_path):
                os.remove(output_path)
                print(f"{video_id}: íŒŒì¼ ì‚­ì œ ì™„ë£Œ")
        except:
            pass

    return result

# ë°°ì¹˜ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
def chunks(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

def safe_cleanup():
    try:
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜[2]
        gc.enable()
        gc.collect()
        gc.disable()
        
        print("ì•ˆì „í•œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ[2]")
    except Exception as e:
        print(f"ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì˜¤ë¥˜: {e}")

def safe_signal_handler(signum, frame):
    print(f"\nì¢…ë£Œ ì‹ í˜¸ ìˆ˜ì‹  ({signum}). ì•ˆì „í•˜ê²Œ ì •ë¦¬ ì¤‘...")
    safe_cleanup()
    sys.exit(0)

signal.signal(signal.SIGINT, safe_signal_handler)
signal.signal(signal.SIGTERM, safe_signal_handler)

def cpu_stage_main():
    print(f"CPU ë°°ì¹˜ í¬ê¸°: {CPU_BATCH_SIZE}ê°œì”©")
    print("CSV ì €ì¥ ì‹œë§ˆë‹¤ ë©”ëª¨ë¦¬ ë¦¬ì…‹")
    
    # ì¸ë±ìŠ¤ ìˆœì„œë¡œ URL ë¡œë“œ
    url_list = load_urls_in_index_order()
    if not url_list:
        print("URL ë¡œë“œ ì‹¤íŒ¨")
        return False
    
    # ì²˜ë¦¬ëœ ID í™•ì¸
    processed_ids = get_processed_video_ids(CPU_OUTPUT_FILE)
    
    # ì¸ë±ìŠ¤ ìˆœì„œëŒ€ë¡œ ë¯¸ì²˜ë¦¬ URL í•„í„°ë§
    unprocessed_urls = [url_data for url_data in url_list if url_data['video_id'] not in processed_ids]
    print(f"CPU ì²˜ë¦¬ ëŒ€ìƒ: {len(unprocessed_urls):,}ê°œ (ì¸ë±ìŠ¤ ìˆœì„œ ìœ ì§€)")

    if len(unprocessed_urls) == 0:
        print("CPU ì²˜ë¦¬ ëª¨ë“  ì™„ë£Œ")
        return True

    start = time.time()
    batch_results = []
    batch_count = 0

    # CPU ë°°ì¹˜ 15ê°œì”© ì²˜ë¦¬
    print(f"CPU ë°°ì¹˜ {CPU_BATCH_SIZE}ê°œì”© ì²˜ë¦¬ ì‹œì‘...")
    try:
        with ThreadPoolExecutor(max_workers=CPU_BATCH_SIZE) as executor:
            url_batches = list(chunks(unprocessed_urls, CPU_BATCH_SIZE))
            
            for batch_idx, url_batch in enumerate(tqdm(url_batches, desc=f"CPU {CPU_BATCH_SIZE}ê°œì”©")):
                # ê° ë°°ì¹˜ë¥¼ ë³‘ë ¬ ì²˜ë¦¬
                futures = [executor.submit(process_cpu_audio_features_only, url_data) for url_data in url_batch]
                
                # ë°°ì¹˜ ë‚´ ê²°ê³¼ ìˆ˜ì§‘
                batch_results_temp = []
                for future in as_completed(futures):
                    try:
                        result = future.result(timeout=180)
                        batch_results_temp.append(result)
                        
                    except Exception as e:
                        print(f"CPU ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                
                batch_results.extend(batch_results_temp)
                
                # ì „ì²´ ì •ë°€ë„ë¡œ ë°°ì¹˜ í¬ê¸° 15ê°œì”© ì €ì¥ + ë©”ëª¨ë¦¬ ë¦¬ì…‹
                if len(batch_results) >= SAVE_BATCH_SIZE:
                    temp_path = os.path.join(TEMP_DIR, f"cpu_features_{batch_count:04d}.csv")
                    
                    # ìˆ˜ì •ëœ ì „ì²´ ì •ë°€ë„ë¡œ ì €ì¥
                    df_temp = pd.DataFrame(batch_results[:SAVE_BATCH_SIZE])
                    save_csv_with_full_precision(df_temp, temp_path)
                    
                    # CSV ì €ì¥ í›„ ë©”ëª¨ë¦¬ ë¦¬ì…‹
                    reset_memory_after_csv_save()
                    
                    batch_results = batch_results[SAVE_BATCH_SIZE:]
                    batch_count += 1
                    
                    print(f"CPU ë°°ì¹˜ {batch_count} ì „ì²´ ì •ë°€ë„ ì €ì¥ ì™„ë£Œ + ë©”ëª¨ë¦¬ ë¦¬ì…‹")
                
                print(f"CPU ë°°ì¹˜ {batch_idx + 1}/{len(url_batches)} ì™„ë£Œ (ì²˜ë¦¬: {len(batch_results_temp)}ê°œ)")

        # ë§ˆì§€ë§‰ ë°°ì¹˜ë„ ì „ì²´ ì •ë°€ë„ ì €ì¥ + ë©”ëª¨ë¦¬ ë¦¬ì…‹
        if batch_results:
            temp_path = os.path.join(TEMP_DIR, f"cpu_features_{batch_count:04d}.csv")
            df_temp = pd.DataFrame(batch_results)
            save_csv_with_full_precision(df_temp, temp_path)
            
            # ë§ˆì§€ë§‰ ë°°ì¹˜ë„ ë©”ëª¨ë¦¬ ë¦¬ì…‹
            reset_memory_after_csv_save()
            print(f"CPU ë§ˆì§€ë§‰ ë°°ì¹˜ ì „ì²´ ì •ë°€ë„ ì €ì¥ ì™„ë£Œ + ë©”ëª¨ë¦¬ ë¦¬ì…‹")

    except Exception as e:
        print(f"CPU ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return False
    finally:
        safe_cleanup()

    elapsed_time = time.time() - start
    print(f"CPU ì²˜ë¦¬ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
    
    # CPU ê²°ê³¼ ë³‘í•© + ë©”ëª¨ë¦¬ ë¦¬ì…‹
    merge_cpu_batches()
    return True

def gpu_stage_main():
    print("2ë‹¨ê³„: GPU ê°ì • ë¶„ì„ ì‹œì‘")
    print(f"GPU ë°°ì¹˜ í¬ê¸°: {GPU_BATCH_SIZE}ê°œì”©")
    print("CSV ì €ì¥ ì‹œë§ˆë‹¤ ë©”ëª¨ë¦¬ ë¦¬ì…‹[2]")
    
    # CPU ê²°ê³¼ íŒŒì¼ í™•ì¸
    if not os.path.exists(CPU_OUTPUT_FILE):
        print(f"CPU ê²°ê³¼ íŒŒì¼ ì—†ìŒ: {CPU_OUTPUT_FILE}")
        return False
    
    # GPU ëª¨ë¸ ì´ˆê¸°í™”
    if not music_emotion_analyzer.initialize_emotion_models():
        print("GPU ê°ì • ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
        return False
    
    # CPU ê²°ê³¼ë¥¼ ì „ì²´ ì •ë°€ë„ë¡œ ì¸ë±ìŠ¤ ìˆœì„œë¡œ ë¡œë“œ
    cpu_df = read_csv_with_full_precision(CPU_OUTPUT_FILE)
    cpu_df = cpu_df[cpu_df['error'].isna()]
    
    # ì¸ë±ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    if 'index' in cpu_df.columns:
        cpu_df = cpu_df.sort_values('index')
        print(f"GPU ì²˜ë¦¬ ëŒ€ìƒ: {len(cpu_df):,}ê°œ (ì¸ë±ìŠ¤ ìˆœì„œ ì •ë ¬)")
    else:
        print(f"GPU ì²˜ë¦¬ ëŒ€ìƒ: {len(cpu_df):,}ê°œ (ì¸ë±ìŠ¤ ì •ë³´ ì—†ìŒ)")

    # ì²˜ë¦¬ëœ ID í™•ì¸
    processed_ids = get_processed_video_ids(GPU_OUTPUT_FILE)
    
    unprocessed_df = cpu_df[~cpu_df['video_id'].isin(processed_ids)]
    print(f"GPU ë¯¸ì²˜ë¦¬ ëŒ€ìƒ: {len(unprocessed_df):,}ê°œ")

    if len(unprocessed_df) == 0:
        print("GPU ì²˜ë¦¬ ëª¨ë“  ì™„ë£Œ")
        return True

    start = time.time()
    batch_results = []
    batch_count = 0

    # GPU ë°°ì¹˜ 5ê°œì”© ì²˜ë¦¬
    print(f"GPU ë°°ì¹˜ {GPU_BATCH_SIZE}ê°œì”© ì²˜ë¦¬ ì‹œì‘...")
    try:
        # GPUëŠ” ìˆœì°¨ ì²˜ë¦¬
        video_batches = list(chunks(unprocessed_df.to_dict('records'), GPU_BATCH_SIZE))
        
        for batch_idx, video_batch in enumerate(tqdm(video_batches, desc=f"ğŸ® GPU {GPU_BATCH_SIZE}ê°œì”©")):
            batch_results_temp = []
            
            for video_data in video_batch:
                try:
                    result = process_gpu_emotion_only(video_data)
                    batch_results_temp.append(result)
                    
                except Exception as e:
                    print(f"GPU ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    batch_results_temp.append({
                        "video_id": video_data['video_id'], 
                        "emotion_name": None, "emotion_label": None, 
                        "confidence": None, "error": str(e)
                    })
            
            batch_results.extend(batch_results_temp)
            
            # ì „ì²´ ì •ë°€ë„ë¡œ ë°°ì¹˜ í¬ê¸° 5ê°œì”© ì €ì¥ + ë©”ëª¨ë¦¬ ë¦¬ì…‹
            if len(batch_results) >= GPU_BATCH_SIZE:
                temp_path = os.path.join(TEMP_DIR, f"gpu_emotions_{batch_count:04d}.csv")
                
                # ì „ì²´ ì •ë°€ë„ë¡œ ì €ì¥
                df_temp = pd.DataFrame(batch_results[:GPU_BATCH_SIZE])
                save_csv_with_full_precision(df_temp, temp_path)
                
                # CSV ì €ì¥ í›„ ë©”ëª¨ë¦¬ ë¦¬ì…‹
                reset_memory_after_csv_save()
                
                batch_results = batch_results[GPU_BATCH_SIZE:]
                batch_count += 1
                
                print(f"GPU ë°°ì¹˜ {batch_count} ì „ì²´ ì •ë°€ë„ ì €ì¥ ì™„ë£Œ + ë©”ëª¨ë¦¬ ë¦¬ì…‹")
            
            print(f"GPU ë°°ì¹˜ {batch_idx + 1}/{len(video_batches)} ì™„ë£Œ (ì²˜ë¦¬: {len(batch_results_temp)}ê°œ)")

        # ë§ˆì§€ë§‰ ë°°ì¹˜ë„ ì „ì²´ ì •ë°€ë„ ì €ì¥ + ë©”ëª¨ë¦¬ ë¦¬ì…‹
        if batch_results:
            temp_path = os.path.join(TEMP_DIR, f"gpu_emotions_{batch_count:04d}.csv")
            df_temp = pd.DataFrame(batch_results)
            save_csv_with_full_precision(df_temp, temp_path)
            
            # ë§ˆì§€ë§‰ ë°°ì¹˜ë„ ë©”ëª¨ë¦¬ ë¦¬ì…‹
            reset_memory_after_csv_save()
            print(f"GPU ë§ˆì§€ë§‰ ë°°ì¹˜ ì „ì²´ ì •ë°€ë„ ì €ì¥ ì™„ë£Œ + ë©”ëª¨ë¦¬ ë¦¬ì…‹")

    except Exception as e:
        print(f"GPU ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return False
    finally:
        safe_cleanup()

    elapsed_time = time.time() - start
    print(f"GPU ì²˜ë¦¬ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
    
    # GPU ê²°ê³¼ ë³‘í•© + ë©”ëª¨ë¦¬ ë¦¬ì…‹
    merge_gpu_batches()
    return True

def merge_cpu_batches():
    import glob
    
    temp_files = sorted(glob.glob(os.path.join(TEMP_DIR, "cpu_features_*.csv")))
    if not temp_files:
        print("CPU ë³‘í•©í•  íŒŒì¼ ì—†ìŒ")
        return

    try:
        df_list = []
        for f in temp_files:
            df_temp = read_csv_with_full_precision(f)
            df_list.append(df_temp)
        
        df_all = pd.concat(df_list, ignore_index=True)
        
        # CPU ì»¬ëŸ¼ ìˆœì„œ
        cpu_columns = [
            'index', 'video_id', 'url', 'pitch_mean', 'energy_mean', 'centroid_mean', 
            'bmp', 'speech_rate', 'initial_silence', 'error'
        ]
        df_all = df_all.reindex(columns=[col for col in cpu_columns if col in df_all.columns])
        
        # ì¸ë±ìŠ¤ ìˆœì„œë¡œ ì •ë ¬
        if 'index' in df_all.columns:
            df_all = df_all.sort_values('index')
            print("CPU ê²°ê³¼ë¥¼ ì¸ë±ìŠ¤ ìˆœì„œë¡œ ì •ë ¬")
        
        # ğŸ”¢ ìˆ˜ì •ëœ ì „ì²´ ì •ë°€ë„ë¡œ ìµœì¢… ì €ì¥[1][3]
        save_csv_with_full_precision(df_all, CPU_OUTPUT_FILE)
        print(f"CPU ê²°ê³¼ ì „ì²´ ì •ë°€ë„ ë³‘í•© ì™„ë£Œ: {len(df_all):,}ê°œ ë ˆì½”ë“œ")
        
        # ë³‘í•© ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ ë¦¬ì…‹
        reset_memory_after_csv_save()
        
        # CPU ì²˜ë¦¬ í†µê³„
        success_count = len(df_all[df_all['error'].isna()])
        error_count = len(df_all[df_all['error'].notna()])
        print(f"CPU ì²˜ë¦¬ í†µê³„: ì„±ê³µ {success_count}ê°œ, ì‹¤íŒ¨ {error_count}ê°œ")
                
    except Exception as e:
        print(f"CPU ë³‘í•© ì˜¤ë¥˜: {e}")

def merge_gpu_batches():
    import glob
    
    temp_files = sorted(glob.glob(os.path.join(TEMP_DIR, "gpu_emotions_*.csv")))
    if not temp_files:
        print("GPU ë³‘í•©í•  íŒŒì¼ ì—†ìŒ")
        return

    try:
        # ì „ì²´ ì •ë°€ë„ ìœ ì§€í•˜ì—¬ ë¡œë“œ
        df_list = []
        for f in temp_files:
            df_temp = read_csv_with_full_precision(f)
            df_list.append(df_temp)
        
        df_all = pd.concat(df_list, ignore_index=True)
        
        # GPU ì»¬ëŸ¼ ìˆœì„œ
        gpu_columns = [
            'video_id', 'emotion_name', 'emotion_label', 'confidence', 'error'
        ]
        df_all = df_all.reindex(columns=gpu_columns)
        
        # ìˆ˜ì •ëœ ì „ì²´ ì •ë°€ë„ë¡œ ì €ì¥[1][3]
        save_csv_with_full_precision(df_all, GPU_OUTPUT_FILE)
        print(f"GPU ê²°ê³¼ ì „ì²´ ì •ë°€ë„ ë³‘í•© ì™„ë£Œ: {len(df_all):,}ê°œ ë ˆì½”ë“œ")
        
        # ë³‘í•© ì™„ë£Œ í›„ ë©”ëª¨ë¦¬ ë¦¬ì…‹
        reset_memory_after_csv_save()
        
        # GPU ì²˜ë¦¬ í†µê³„
        success_count = len(df_all[df_all['error'].isna()])
        error_count = len(df_all[df_all['error'].notna()])
        print(f"GPU ì²˜ë¦¬ í†µê³„: ì„±ê³µ {success_count}ê°œ, ì‹¤íŒ¨ {error_count}ê°œ")
        
        # ê°ì • ë¶„í¬
        if 'emotion_name' in df_all.columns:
            emotion_counts = df_all[df_all['error'].isna()]['emotion_name'].value_counts()
            print("ê°ì • ë¶„í¬:")
            for emotion, count in emotion_counts.items():
                percentage = (count / success_count) * 100 if success_count > 0 else 0
                print(f"   {emotion}: {count}ê°œ ({percentage:.1f}%)")
                
    except Exception as e:
        print(f"GPU ë³‘í•© ì˜¤ë¥˜: {e}")

def final_merge_csv_files():
    print("3ë‹¨ê³„: ìµœì¢… CSV ë³‘í•© (video_id ê¸°ì¤€) + ì „ì²´ ì •ë°€ë„ + ë©”ëª¨ë¦¬ ë¦¬ì…‹")
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(CPU_OUTPUT_FILE):
        print(f"CPU ê²°ê³¼ íŒŒì¼ ì—†ìŒ: {CPU_OUTPUT_FILE}")
        return False
    
    if not os.path.exists(GPU_OUTPUT_FILE):
        print(f"GPU ê²°ê³¼ íŒŒì¼ ì—†ìŒ: {GPU_OUTPUT_FILE}")
        return False
    
    try:
        # ì „ì²´ ì •ë°€ë„ ìœ ì§€í•˜ì—¬ CSV íŒŒì¼ ë¡œë“œ
        cpu_df = read_csv_with_full_precision(CPU_OUTPUT_FILE)
        gpu_df = read_csv_with_full_precision(GPU_OUTPUT_FILE)
        
        print(f"CPU ë°ì´í„°: {len(cpu_df):,}ê°œ")
        print(f"GPU ë°ì´í„°: {len(gpu_df):,}ê°œ")
        
        # video_id ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
        merged_df = pd.merge(
            cpu_df[cpu_df['error'].isna()],
            gpu_df[gpu_df['error'].isna()],
            on='video_id', 
            how='inner'
        )
        
        # ì¸ë±ìŠ¤ ìˆœì„œë¡œ ì •ë ¬
        if 'index' in merged_df.columns:
            merged_df = merged_df.sort_values('index')
            print("ìµœì¢… ê²°ê³¼ë¥¼ ì¸ë±ìŠ¤ ìˆœì„œë¡œ ì •ë ¬")
        
        # ìµœì¢… ì»¬ëŸ¼ ìˆœì„œ
        final_columns = [
            'video_id', 'pitch_mean', 'energy_mean', 'centroid_mean', 
            'bmp', 'speech_rate', 'initial_silence', 
            'emotion_name', 'emotion_label', 'confidence'
        ]
        merged_df = merged_df.reindex(columns=final_columns)
        
        # ìµœì¢… ì „ì²´ ì •ë°€ë„ë¡œ íŒŒì¼ ì €ì¥
        save_csv_with_full_precision(merged_df, FINAL_OUTPUT_FILE)
        
        # ìµœì¢… ì €ì¥ í›„ ë©”ëª¨ë¦¬ ë¦¬ì…‹
        reset_memory_after_csv_save()
        
        print(f"ìµœì¢… ì „ì²´ ì •ë°€ë„ ë³‘í•© ì™„ë£Œ: {len(merged_df):,}ê°œ ë ˆì½”ë“œ â†’ {FINAL_OUTPUT_FILE}[3]")
        
        # ìµœì¢… í†µê³„
        print(f"ì„±ê³µì ìœ¼ë¡œ ë³‘í•©ëœ ë ˆì½”ë“œ: {len(merged_df):,}ê°œ")
        print(f"í‰ê·  ì‹ ë¢°ë„: {merged_df['confidence'].mean()}")
        
        # ê°ì • ë¶„í¬ ìµœì¢…
        emotion_counts = merged_df['emotion_name'].value_counts()
        print("ìµœì¢… ê°ì • ë¶„í¬:")
        for emotion, count in emotion_counts.items():
            percentage = (count / len(merged_df)) * 100
            print(f"   {emotion}: {count}ê°œ ({percentage:.1f}%)")
        
        return True
                
    except Exception as e:
        print(f"ìµœì¢… ë³‘í•© ì˜¤ë¥˜: {e}")
        return False

def main():
    print("1ë‹¨ê³„: CPU ë‹¤ìš´ë¡œë“œ + ê¸°ë³¸ ì •ë³´ (ë°°ì¹˜ 15ê°œ)")
    print("2ë‹¨ê³„: GPU ê°ì • ë¶„ì„ (ë°°ì¹˜ 5ê°œ)")
    print("3ë‹¨ê³„: ìµœì¢… ë³‘í•©")
    print("ëª¨ë“  CSV ì €ì¥ ì‹œ ë©”ëª¨ë¦¬ ë¦¬ì…‹ ì ìš©")
    print("ì „ì²´ ì†Œìˆ˜ì  ì •ë°€ë„ ì €ì¥")
    
    # CPU ì²˜ë¦¬
    if not cpu_stage_main():
        print("1ë‹¨ê³„ CPU ì²˜ë¦¬ ì‹¤íŒ¨")
        return
    
    # GPU ì²˜ë¦¬
    if not gpu_stage_main():
        print("2ë‹¨ê³„ GPU ì²˜ë¦¬ ì‹¤íŒ¨")
        return
    
    # ìµœì¢… ë³‘í•©
    if not final_merge_csv_files():
        print("3ë‹¨ê³„ ìµœì¢… ë³‘í•© ì‹¤íŒ¨")
        return
    
    print(f"ìµœì¢… ê²°ê³¼: {FINAL_OUTPUT_FILE}")

if __name__ == "__main__":
    main()
