"""
ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ëª¨ë“ˆ
GPU ì„¤ì •, CSV ì €ì¥/ë¡œë“œ ë“±ì˜ ê³µí†µ ê¸°ëŠ¥ ì œê³µ
"""

import os
import pandas as pd
import numpy as np
import torch
import tensorflow as tf
import gc
from typing import Optional


def setup_gpu():
    """
    GPU ì„¤ì • ë° ìµœì í™”
    TensorFlowì™€ PyTorch GPU í™˜ê²½ì„ ìë™ìœ¼ë¡œ ì„¤ì •
    
    Returns:
        dict: GPU ì„¤ì • ì •ë³´
    """
    gpu_info = {
        "tensorflow_gpu": False,
        "pytorch_cuda": False,
        "device": "cpu"
    }
    
    # TensorFlow GPU ì„¤ì •
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print(f"âœ“ TensorFlow GPU í™œì„±í™”: {len(gpus)}ê°œ")
            gpu_info["tensorflow_gpu"] = True
        except RuntimeError as e:
            print(f"âš  TensorFlow GPU ì„¤ì • ì˜¤ë¥˜: {e}")
    
    # PyTorch CUDA ì„¤ì •
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        print(f"âœ“ PyTorch CUDA í™œì„±í™”: {torch.cuda.get_device_name(0)}")
        print(f"âœ“ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        gpu_info["pytorch_cuda"] = True
        gpu_info["device"] = "cuda:0"
    else:
        print("âš  CUDA ì‚¬ìš© ë¶ˆê°€, CPU ì‚¬ìš©")
        gpu_info["device"] = "cpu"
    
    return gpu_info


def save_csv_safely(df: pd.DataFrame, filepath: str, encoding: str = "utf-8-sig") -> bool:
    """
    ì „ì²´ ì •ë°€ë„ë¥¼ ìœ ì§€í•˜ë©´ì„œ CSV íŒŒì¼ë¡œ ì•ˆì „í•˜ê²Œ ì €ì¥
    
    Args:
        df: ì €ì¥í•  DataFrame
        filepath: ì €ì¥ ê²½ë¡œ
        encoding: ì¸ì½”ë”© ë°©ì‹ (ê¸°ë³¸ê°’: utf-8-sig)
    
    Returns:
        bool: ì €ì¥ ì„±ê³µ ì—¬ë¶€
    """
    try:
        # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        # ì „ì²´ ì •ë°€ë„ ìœ ì§€í•˜ë©° ì €ì¥
        df.to_csv(filepath, index=False, encoding=encoding)
        print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {filepath}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        reset_memory()
        return True
    except Exception as e:
        print(f"âŒ CSV ì €ì¥ ì‹¤íŒ¨ ({filepath}): {e}")
        return False


def load_csv_safely(filepath: str, encoding: str = "utf-8") -> Optional[pd.DataFrame]:
    """
    ì „ì²´ ì •ë°€ë„ë¥¼ ìœ ì§€í•˜ë©´ì„œ CSV íŒŒì¼ ë¡œë“œ
    
    Args:
        filepath: ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ
        encoding: ì¸ì½”ë”© ë°©ì‹ (ê¸°ë³¸ê°’: utf-8)
    
    Returns:
        DataFrame ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
    """
    try:
        # float_precision='round_trip'ìœ¼ë¡œ ì •ë°€ë„ ìœ ì§€
        df = pd.read_csv(filepath, encoding=encoding, float_precision='round_trip')
        print(f"âœ… CSV ë¡œë“œ ì™„ë£Œ: {filepath} ({len(df)} rows)")
        return df
    except Exception as e:
        print(f"âŒ CSV ë¡œë“œ ì‹¤íŒ¨ ({filepath}): {e}")
        return None


def reset_memory():
    """
    ë©”ëª¨ë¦¬ ì •ë¦¬ (Python GC + GPU ìºì‹œ)
    CSV ì €ì¥ í›„ ë˜ëŠ” ëŒ€ìš©ëŸ‰ ì‘ì—… í›„ í˜¸ì¶œ
    """
    try:
        # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.enable()
        collected = gc.collect()
        gc.disable()
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
            torch.cuda.ipc_collect()
        
        print(f"ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: {collected}ê°œ ê°ì²´ ì œê±°")
        return True
    except Exception as e:
        print(f"âš  ë©”ëª¨ë¦¬ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        return False


def extract_video_id(url: str) -> Optional[str]:
    """
    YouTube URLì—ì„œ video_id ì¶”ì¶œ
    
    Args:
        url: YouTube ì˜ìƒ URL
    
    Returns:
        video_id (11ìë¦¬) ë˜ëŠ” None
    """
    from urllib.parse import urlparse, parse_qs
    
    try:
        parsed = urlparse(url)
        video_id = parse_qs(parsed.query).get("v", [None])[0]
        return video_id
    except Exception:
        return None


def setup_multiprocessing_optimized():
    """
    ë©€í‹°í”„ë¡œì„¸ì‹± ìµœì í™” ì„¤ì •
    ë°°ì¹˜ ì²˜ë¦¬ ì‹œ í˜¸ì¶œ
    """
    import multiprocessing as mp
    
    try:
        mp.set_start_method('spawn', force=True)
        
        # ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” í™˜ê²½ ë³€ìˆ˜
        os.environ.update({
            "OMP_NUM_THREADS": "4",
            "MKL_NUM_THREADS": "4",
            "NUMEXPR_NUM_THREADS": "4",
            "OPENBLAS_NUM_THREADS": "4",
            "TOKENIZERS_PARALLELISM": "false",
            "PYTORCH_CUDA_ALLOC_CONF": "max_split_size_mb:256",
            "CUDA_LAUNCH_BLOCKING": "0",
            "PYTHONWARNINGS": "ignore:semaphore_tracker:UserWarning"
        })
        
        print("âœ“ ë©€í‹°í”„ë¡œì„¸ì‹± ìµœì í™” ì„¤ì • ì™„ë£Œ")
        return True
    except Exception as e:
        print(f"âš  ë©€í‹°í”„ë¡œì„¸ì‹± ì„¤ì • ì‹¤íŒ¨: {e}")
        return False
